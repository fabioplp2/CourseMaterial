{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TensorFlow™ is an open source software library for high performance numerical computation. Its flexible architecture allows easy deployment of computation across a variety of platforms using (CPUs, GPUs, TPUs (TEnsor Processing Units)), and from desktops to clusters of servers to mobile and edge devices [1].\n",
    "\n",
    "Originally developed by researchers and engineers from the Google Brain team within Google’s AI organization, it comes with strong support for machine learning and deep learning and the flexible numerical computation core is used across many other scientific domains.\n",
    "\n",
    "TensorFlow is an interface to express machine learning algorithms and provides an implementation for executing such algorithms. In this regard, TensoFlow is not a package/API that can be used to pass your training data and get a predictor/classifier/regression tool. Instead, it is a lower level library that provides features required to implement machine learning algorithms in an optimal way. Also, there are predefined implementations of algorithms that a user can use if the flexibility of designing the implementation is not desired. \n",
    "\n",
    "TensorFlow is designed to facilitate scaling your application by separating the language interface and the execution allowing the code to be executed as a prototype on your laptop or the final product on a cluster.\n",
    "\n",
    "*Language Interface:*\n",
    "\n",
    "    Python, C++, ...\n",
    "\n",
    "*Execution Environment:*\n",
    "\n",
    "    Execution master to choose Local CPU, GPU or a master node on a cluster\n",
    "\n",
    "Source of the name TensorFlow: N dimensional arrays aka Tensors along with mathematical operations are the base of the computation graph executed by TensorFlow.\n",
    "\n",
    "\n",
    "**References:**\n",
    "\n",
    "http://TensorFlow.org\n",
    "\n",
    "https://app.pluralsight.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "The following factors can affect the installation:\n",
    "    - OS\n",
    "    - GPU (cuda enabaled) \n",
    "    - Environment (Direct, Virtual, Docer Container)\n",
    "    - Python version (assumin that python is used for the programming language)\n",
    "        Linux, MacOS --> 2.7, 3.3 or later\n",
    "        windows --> 3.5 or later\n",
    "    \n",
    "**For graphic cards info:**\n",
    "\n",
    "```shell\n",
    "$ dxdiag\n",
    "```\n",
    "or use chrome:gpu on your chrome browser\n",
    "\n",
    "**For python version:**\n",
    "\n",
    "```python\n",
    "import sys\n",
    "sys.version\n",
    "```\n",
    "\n",
    "**Installation using pip:**\n",
    "\n",
    "```shell\n",
    "pip install --upgrade tensorflow -- Chose the cpu version\n",
    "```\n",
    "\n",
    "Note that on tensorflow.org you have the option to download the source file from github and config it on your machine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:54.681760Z",
     "start_time": "2019-03-16T17:23:54.667835Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:57.145167Z",
     "start_time": "2019-03-16T17:23:54.683753Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Avoid some deprecation warnings\n",
    "import tensorflow as tf\n",
    "print('TensorFlow version {}'.format(tf.__version__))\n",
    "print(tf.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:57.172088Z",
     "start_time": "2019-03-16T17:23:57.148121Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.Session() # Starting a local session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:57.194005Z",
     "start_time": "2019-03-16T17:23:57.175051Z"
    }
   },
   "outputs": [],
   "source": [
    "a = tf.constant(20)\n",
    "b = tf.constant(50)\n",
    "print('a*b = {}'.format(sess.run(a*b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:57.207975Z",
     "start_time": "2019-03-16T17:23:57.196026Z"
    }
   },
   "outputs": [],
   "source": [
    "a.eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Example\n",
    "\n",
    "A simple example of predictinh house proce based on as single variable/feature of house size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:57.901806Z",
     "start_time": "2019-03-16T17:23:57.210955Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "import pygeostat as gs # http://www.ccgalberta.com/pygeostat/welcome.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:57.910751Z",
     "start_time": "2019-03-16T17:23:57.902735Z"
    }
   },
   "outputs": [],
   "source": [
    "num_house = 200\n",
    "np.random.seed(69067)\n",
    "house_size = np.random.randint(1000, 3500, num_house)\n",
    "data_housing = pd.DataFrame(columns=['HouseSize'], data=house_size)\n",
    "\n",
    "data_housing['HousePrice'] = data_housing['HouseSize'].apply(lambda size: size* 200 + np.random.randint(10000, 60000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:58.005462Z",
     "start_time": "2019-03-16T17:23:57.912710Z"
    }
   },
   "outputs": [],
   "source": [
    "data_housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:58.452267Z",
     "start_time": "2019-03-16T17:23:58.007458Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, figsize=(11,5))\n",
    "# Simple scat plot\n",
    "data_housing.plot(x='HouseSize', y='HousePrice', kind='scatter',ax=ax[0], color='gray')\n",
    "# Scatter plot with bivariate PDF highlight\n",
    "gs.scatplt(data_housing.HouseSize, data_housing.HousePrice, ax=ax[1], cbar=True)\n",
    "fig.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latext\n",
    "The price was generated based on a simple equation:\n",
    "\n",
    "$ Price = SizeFactor \\times Size + PriceOffset $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:58.465291Z",
     "start_time": "2019-03-16T17:23:58.454262Z"
    }
   },
   "outputs": [],
   "source": [
    "size_mean = np.mean(data_housing['HouseSize'])\n",
    "size_std = np.std(data_housing['HouseSize'])\n",
    "data_housing['HouseSize_std'] = data_housing['HouseSize'].apply(lambda x: (x - size_mean)/ size_std )\n",
    "\n",
    "price_mean = np.mean(data_housing['HousePrice'])\n",
    "price_std = np.std(data_housing['HousePrice'])\n",
    "data_housing['HousePrice_std'] = data_housing['HousePrice'].apply(lambda x: (x - price_mean)/ price_std)\n",
    "\n",
    "data_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:58.789364Z",
     "start_time": "2019-03-16T17:23:58.467227Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, figsize=(11,5))\n",
    "# Simple scat plot\n",
    "data_housing.plot(x='HouseSize_std', y='HousePrice_std', kind='scatter',ax=ax[0], color='gray')\n",
    "# Scatter plot with bivariate PDF highlight\n",
    "gs.scatplt(data_housing.HouseSize_std, data_housing.HousePrice_std, ax=ax[1], cbar=True)\n",
    "fig.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:58.797343Z",
     "start_time": "2019-03-16T17:23:58.791361Z"
    }
   },
   "outputs": [],
   "source": [
    "tarin_portion = 0.7\n",
    "train_mask = np.random.rand(len(data_housing)) < tarin_portion\n",
    "data_housing_train = data_housing[train_mask].reset_index(drop=True)\n",
    "num_tarining_data = len(data_housing_train)\n",
    "data_housing_test = data_housing[~train_mask].reset_index(drop=True)\n",
    "print('Number of training data: {} \\nNumber of test data: {}'.format(len(data_housing_train), len(data_housing_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:58.811307Z",
     "start_time": "2019-03-16T17:23:58.800336Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect training data\n",
    "data_housing_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T20:26:05.220335Z",
     "start_time": "2019-01-20T20:26:05.206376Z"
    }
   },
   "source": [
    "## Use tensor flow\n",
    "Note about tensor types:\n",
    "\n",
    "- Constant: Constant value that does not change\n",
    "- Variable: Value adjusted in the computation graph of TensorFlow\n",
    "- PlaceHodler: Used to pass data into the graph (e.g. from an external source)\n",
    "\n",
    "Also, the following concepts:\n",
    "- Model: A set of mathematical operations supported by tensor flow that defined the neural network part of the computation graph\n",
    "- Cost function: The target function to optimize by calculation the error and modify the weight through back propagation steps\n",
    "- Optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:58.822319Z",
     "start_time": "2019-03-16T17:23:58.813302Z"
    }
   },
   "outputs": [],
   "source": [
    "# DEfine the placeholders that get updated as our model gets optimized\n",
    "tf_house_size = tf.placeholder(dtype='float', name='HouseSize')\n",
    "tf_house_price =tf.placeholder(dtype='float', name='HousePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:58.837274Z",
     "start_time": "2019-03-16T17:23:58.824276Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define variables that get modified as we train our model based on the provided data\n",
    "tf_size_factor = tf.Variable(np.random.randn(), name = 'SizeFactor')\n",
    "tf_price_offset = tf.Variable(np.random.randn(), name = 'PriceOffset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:58.845272Z",
     "start_time": "2019-03-16T17:23:58.839235Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the operation/model that needs to be trained to provide optimal result (minimum error based on the training data)\n",
    "tf_price_predict = tf.add(tf.multiply(tf_size_factor, tf_house_size), tf_price_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:58.859180Z",
     "start_time": "2019-03-16T17:23:58.846213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a cost fucntion that can be used to tarin the model i.e. mean squer error\n",
    "tf_cost = tf.reduce_sum(tf.pow(tf.subtract(tf_price_predict,tf_house_price),2))/num_tarining_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:58.945005Z",
     "start_time": "2019-03-16T17:23:58.862170Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "optimizer = optimizer.minimize(tf_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:59.070645Z",
     "start_time": "2019-03-16T17:23:58.946944Z"
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "dispaly_freq = 5\n",
    "num_train_iterations = 50\n",
    "\n",
    "size_factor_calc = []\n",
    "price_offset_calc = []\n",
    "training_cost = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for iteration in range(num_train_iterations):\n",
    "        \n",
    "        # This works becuase of the reduce sum over all data\n",
    "        sess.run(optimizer, feed_dict={tf_house_size: data_housing_train['HouseSize_std'],\n",
    "                                             tf_house_price: data_housing_train['HousePrice_std']})\n",
    "        \n",
    "#         # Alternatively, each training record may be passed separately (TensorFlow optimizer remembers)\n",
    "#         for (x,Y) in zip(data_housing_train['HouseSize_std'], data_housing_train['HousePrice_std']): # Note that palce holders need to be fed\n",
    "#             sess.run(optimizer, feed_dict={tf_house_size: x, tf_house_price: Y})\n",
    "        \n",
    "        # Get the cost function value\n",
    "        c = sess.run(tf_cost, feed_dict={tf_house_size: data_housing_train['HouseSize_std'],\n",
    "                                             tf_house_price: data_housing_train['HousePrice_std']})\n",
    "        training_cost.append(c)\n",
    "        # get the size factor and price offset st the current iteration\n",
    "        size_factor_calc.append(sess.run(tf_size_factor))\n",
    "        price_offset_calc.append(sess.run(tf_price_offset))\n",
    "        \n",
    "        if (iteration + 1)%dispaly_freq == 0:\n",
    "            \n",
    "            \n",
    "            print('Iteration number: %03d'%(iteration+1), 'cost = {:.9f}'.format(training_cost[iteration]),\\\n",
    "                 'size factor =', size_factor_calc[iteration], 'price offset=', price_offset_calc[iteration])\n",
    "            \n",
    "    print('Optimization is done!')\n",
    "    print('Final training cost = {:.5f}'.format(training_cost[-1]))\n",
    "    print('Final size factor = ', size_factor_calc[-1])\n",
    "    print('Final price offset = ', price_offset_calc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize gradient descent evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:59.367842Z",
     "start_time": "2019-03-16T17:23:59.071613Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, minax = plt.subplots(1,3,figsize=(15,4))\n",
    "\n",
    "ax = minax[0]\n",
    "ax.plot(training_cost, c = 'r', lw =2)\n",
    "ax.set_xlabel('Iterations'); ax.set_ylabel('Training Cost (Means Squared Error)')\n",
    "ax.grid(True)\n",
    "\n",
    "ax = minax[1]\n",
    "ax.plot(size_factor_calc, c='k', lw=2)\n",
    "ax.set_xlabel('Iterations'); ax.set_ylabel('size factor (standardized)')\n",
    "ax.grid(True)\n",
    "\n",
    "ax = minax[2]\n",
    "ax.plot(price_offset_calc, c='b', lw=2)\n",
    "ax.set_xlabel('Iterations'); ax.set_ylabel('price_offset (standardized)')\n",
    "ax.grid(True)\n",
    "\n",
    "fig.tight_layout(pad = 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:59.618148Z",
     "start_time": "2019-03-16T17:23:59.369836Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(6,6))\n",
    "\n",
    "test_price_predicted = (size_factor_calc[-1]*data_housing_test.HouseSize_std + price_offset_calc[-1]) * price_std + price_mean\n",
    "\n",
    "# Predicted Model\n",
    "ax.plot(data_housing_test.HouseSize, test_price_predicted, c='gray', lw=2, label = 'Predicted')\n",
    "# Actual test data\n",
    "ax.scatter(data_housing_test.HouseSize, data_housing_test.HousePrice, s = 20, label = 'Test Data')\n",
    "\n",
    "mean_squared_error = 0\n",
    "for price_point, size_point, price_pred in zip(data_housing_test.HousePrice, data_housing_test.HouseSize, test_price_predicted):\n",
    "    ax.plot([size_point, size_point], [price_point, price_pred], c='r', lw=1.5, ls ='--')\n",
    "    mean_squared_error += (price_point - price_pred)**2\n",
    "\n",
    "mean_squared_error = mean_squared_error/len(data_housing_test)\n",
    "rmse = np.sqrt(mean_squared_error)\n",
    "\n",
    "ax.text(0.1,.7, 'RMSE = {:.2f}'.format(rmse), transform = ax.transAxes, fontsize=16)\n",
    "\n",
    "ax.set_xlabel('House Size'); ax.set_ylabel('House Price')\n",
    "ax.legend(fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Linear Regression\n",
    "\n",
    "This example can be also solved by simple linear regression. As shown below, the result of gradient descent optimization and regression analysis are very close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:59.626125Z",
     "start_time": "2019-03-16T17:23:59.620142Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data_housing_test.HouseSize\n",
    "y = data_housing_test.HousePrice\n",
    "slope = np.cov(X,y)[0][1]/np.cov(X,y)[0][0] # Covariance(X,y)/Variance(X)\n",
    "intercept = np.mean(y)-slope*np.mean(X) # mean(y) - slope * mean(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:23:59.837561Z",
     "start_time": "2019-03-16T17:23:59.628119Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "# Predicted Model (TensorFlow)\n",
    "ax.plot(data_housing_test.HouseSize, test_price_predicted, c='gray', lw=2, label = 'Predicted TensorFlow')\n",
    "# Actual test data\n",
    "ax.scatter(data_housing_test.HouseSize, data_housing_test.HousePrice, s = 40, label = 'Test Data', facecolors='none', edgecolors='b')\n",
    "# Predicted linear regression\n",
    "ax.plot(data_housing_test.HouseSize, slope*data_housing_test.HouseSize+intercept, c='red', lw=0.5, linestyle=':', label = 'Linear Regression')\n",
    "ax.legend(fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The root mean squared error of the optimized model for test data is shown in the plot above along with deviations of the actual price from the predicted one using the simple linear model. This is a simple linear regression example that was implemented using TensorFlow. Note that while variables rely on the initialization, placeholders are the tensors provided based on data (i.e. external source compared to the computation graph). You need to make sure the provided data result in tensors with dimensions that comply with the operation that you have added to your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor\n",
    "\n",
    "Some introduction to tensor properties help to better understand how TensorFlow works. Note that mathematics is the base of any algorithm developed by humans. In case of tensors, linear algebra/multilinear algebra is of great significance.\n",
    "\n",
    "Tensor may be defined as an n-dimensional array to represent data. For example 500 observations of 4 variable may be a tensor of 4 by 500 or the other way around.\\\n",
    "\n",
    "## Tensor Properties\n",
    "\n",
    "Tensors in an operation need to shared compatible properties. \n",
    "\n",
    "### Rank\n",
    "\n",
    "Dimensionality of a tensor.\n",
    "\n",
    "| Rank        | Object           |\n",
    "| ------------- |:-------------:|\n",
    "| 0      | scalar |\n",
    "| 1      | vector      |\n",
    "| 2 | N by N matrix   |\n",
    "| 3 | 3-Tensor(cube)   |\n",
    "| n | n-Tensor(cube)   |\n",
    "\n",
    "### Shape\n",
    "\n",
    "It is closely related to the rank and also number of data in the tensor\n",
    "\n",
    "example: \n",
    "\n",
    "| Rank        | Object           |  Example | Shape\n",
    "| ------------- |:-------------:|:-------------:| :-------------:| \n",
    "| 0 |Scalar|s = 25 | []| \n",
    "| 2 |Matrix|m = [[1,2,3], [4,5,6]] | [2,3]| \n",
    " \n",
    "### Data Type\n",
    "\n",
    "The following data types are supported by TensorFlow\n",
    "\n",
    "- float32, float64\n",
    "- int8, int16, int32, int64\n",
    "- uint8, uint16\n",
    "- string\n",
    "- bool\n",
    "- complex64, complex128\n",
    "- qint8, qint16\n",
    "\n",
    "qint short for Quantized values. The quantized data type is used to scale data/number to reduce size so it takes less bits. This helps to improve processing power of TensorFlow which is critical for scalability. Google has invested on special processing chips called TensoFlow Processing Units (TPUs) that utilize quantized values. \n",
    "\n",
    "\n",
    "### Tensor Info\n",
    "\n",
    "TensorFlow provides the following methods to inspect a tensor:\n",
    "\n",
    "- get_shape \n",
    "- reshape\n",
    "- rank\n",
    "- dtype\n",
    "- cast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient descent is a popular optimization algorithm that works based on finding sequential steps/vectors in the multivariate space of features that provide the steepest reduction in the loss/cost function or steepest increase in the profit/gain function. The learning rate corresponds to the length of the vectors. The direction is determined based on derivative based on the features. \n",
    "\n",
    "![alt text](Figures/GradientDescent.png \"Obtained from https://www.oreilly.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animate Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:24:00.001863Z",
     "start_time": "2019-03-16T17:23:59.839555Z"
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "dispaly_freq = 5\n",
    "num_train_iterations = 50\n",
    "%matplotlib notebook\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    size_factor_calc = []\n",
    "    price_offset_calc = []\n",
    "    training_cost = []\n",
    "    \n",
    "    \n",
    "    for iteration in range(num_train_iterations):\n",
    "        \n",
    "        # This works becuase of the reduce sum over all data\n",
    "        sess.run(optimizer, feed_dict={tf_house_size: data_housing_train['HouseSize_std'],\n",
    "                                             tf_house_price: data_housing_train['HousePrice_std']})\n",
    "        \n",
    "        # Get the cost function value\n",
    "        c = sess.run(tf_cost, feed_dict={tf_house_size: data_housing_train['HouseSize_std'],\n",
    "                                             tf_house_price: data_housing_train['HousePrice_std']})\n",
    "        training_cost.append(c)\n",
    "        # get the size factor and price offset st the current iteration\n",
    "        size_factor_calc.append(sess.run(tf_size_factor))\n",
    "        price_offset_calc.append(sess.run(tf_price_offset))\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "\n",
    "    \n",
    "    # Actual tarining data\n",
    "    ax.scatter(data_housing_train.HouseSize, data_housing_train.HousePrice, s = 40, facecolors='none', edgecolors='g', label = 'Test Data')\n",
    "    ax.set_xlabel('House Size')\n",
    "    ax.set_ylabel('House Price')\n",
    "    \n",
    "    line, = ax.plot([],[], c='gray') \n",
    "    time_text = ax.text(.1, .8, '', fontsize=11, transform = ax.transAxes, color='r')\n",
    "    fig.tight_layout(pad = 2.0)\n",
    "    \n",
    "    def update(i):\n",
    "        test_price_predicted = (size_factor_calc[i]*data_housing_train.HouseSize_std + price_offset_calc[i]) * price_std + price_mean\n",
    "        line.set_xdata(data_housing_train.HouseSize)\n",
    "        line.set_ydata(test_price_predicted)\n",
    "        time_text.set_text('Cost: {:.4f}'.format(training_cost[i]))\n",
    "        return line, time_text,\n",
    "    \n",
    "    \n",
    "    def initAnim():\n",
    "        test_price_predicted = (size_factor_calc[0]*data_housing_train.HouseSize_std + price_offset_calc[0]) * price_std + price_mean\n",
    "        line.set_ydata(test_price_predicted)\n",
    "        line.set_xdata(data_housing_train.HouseSize)\n",
    "        return line,\n",
    "    \n",
    "    anim = animation.FuncAnimation(fig, update, frames=num_train_iterations, init_func=initAnim, interval=200, blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST example\n",
    "\n",
    "A data set containing 28 by 28 images of handwritten numbers from 0 to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network\n",
    "\n",
    "A simple network with one layer that maps the image pixels into the output classification categories (i.e. classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:45:39.402482Z",
     "start_time": "2019-03-16T20:45:39.398493Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from toolbox.utilities import image_plot, transform_label, flatten_images, mnist_weight_check, mnist_result_anim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data from TensorFlow's tutorial was not used as it has a deprecation warning. Instead, keras datasets was used to get the mnist data set. The main difference is that the image is not flatten and the labels are not classified by an array with max probability i.e. one_hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:45:41.053672Z",
     "start_time": "2019-03-16T20:45:40.581103Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check an example image from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:45:41.370423Z",
     "start_time": "2019-03-16T20:45:41.234812Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "image_plot(x_train[4], y_train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:45:41.881875Z",
     "start_time": "2019-03-16T20:45:41.876925Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:45:43.343443Z",
     "start_time": "2019-03-16T20:45:42.203602Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train, y_test = transform_label(y_train), transform_label(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:45:43.349181Z",
     "start_time": "2019-03-16T20:45:43.343443Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:45:43.367172Z",
     "start_time": "2019-03-16T20:45:43.350179Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:45:43.376108Z",
     "start_time": "2019-03-16T20:45:43.369147Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test = flatten_images(x_train), flatten_images(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:45:43.536073Z",
     "start_time": "2019-03-16T20:45:43.530092Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a helper function for summary reports in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:45:46.495266Z",
     "start_time": "2019-03-16T20:45:46.491240Z"
    }
   },
   "outputs": [],
   "source": [
    "def variable_summary(var):\n",
    "    with tf.name_scope('Summary'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('Mean', mean)\n",
    "        std = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar('Std', std)\n",
    "        \n",
    "        tf.summary.scalar('Min', tf.reduce_min(var))\n",
    "        tf.summary.scalar('Max', tf.reduce_max(var))\n",
    "        tf.summary.histogram('HistoGram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:45:47.240317Z",
     "start_time": "2019-03-16T20:45:47.237316Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:45:48.803215Z",
     "start_time": "2019-03-16T20:45:48.685176Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(name='InputLayer'):\n",
    "    # plcae holder for the input which is a 28 by 28 image data\n",
    "    x = tf.placeholder(tf.float32, shape=[None,784], name='Image')\n",
    "    #place holder for predcicted output\n",
    "    y = tf.placeholder(tf.float32, shape=[None,10], name = 'Classes')\n",
    "\n",
    "# Weights and bias for the only layer of the model\n",
    "with tf.name_scope('Weights'):\n",
    "    w = tf.Variable(tf.zeros([784,10]), name='PixelWeights')\n",
    "    variable_summary(w)\n",
    "with tf.name_scope('Biases'):\n",
    "    b = tf.Variable(tf.zeros([10]), name= 'PixelBiases')\n",
    "    variable_summary(b)\n",
    "\n",
    "with tf.name_scope('SimpleLayer'):\n",
    "    layer1 = tf.matmul(x,w) + b\n",
    "\n",
    "    \n",
    "# Define model (make sure that the tensor rank an shape comply with the operation)\n",
    "with tf.name_scope('Output'):\n",
    "    y_p = tf.nn.softmax(layer1, name='Classes') # softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)\n",
    "\n",
    "# Define the lost that is cross_entropy\n",
    "with tf.name_scope('CrossEntropy'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=layer1), name='SoftMaxCrossEntropy')\n",
    "    # Note: soft max is implmeneted so, logits should be layer1 although using y_p would work too\n",
    "\n",
    "learning_rate = 0.5\n",
    "with tf.name_scope('Optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate, name='GDO').minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the performance measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:45:49.597642Z",
     "start_time": "2019-03-16T20:45:49.586671Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(name='Accuracy'):\n",
    "    correct = tf.equal(tf.argmax(y_p,1), tf.argmax(y,1), name='ArgMaxTransform')\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'AccuracyCalculation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model\n",
    "\n",
    "Note that the initial guess (i.e. zeros or random normal) can significantly affect the performance of the model and depending on the type of optimization algorithm, it may get the model trapped in local optimal solutions for weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:49:59.849386Z",
     "start_time": "2019-03-16T20:49:59.844397Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Log path for tensor board\n",
    "log_path = r'./tensorboard_log/{}'.format(datetime.now().strftime('%Y%m%d-%H%M%S')) # Log path for tensor board \n",
    "try:\n",
    "    shutil.rmtree(log_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print('Use the following command line to check the tensor board web service:\\n\\t tensorboard --logdir={}'.format(log_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:50:13.883153Z",
     "start_time": "2019-03-16T20:50:01.195144Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "batch_list = [ i for i in range(0,len(x_train),batch_size)] # Break training data into batches\n",
    "nump_epoch = 20\n",
    "weight_repo = [] # Get all the weights\n",
    "\n",
    "\n",
    "# Merging summary reports for the computation graph\n",
    "all_summary = tf.summary.merge_all()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    tb_writer = tf.summary.FileWriter(logdir = log_path, graph=sess.graph, session=sess)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer()) # Session initilization\n",
    "    \n",
    "    \n",
    "    for epoch in range(nump_epoch):\n",
    "        for i in range(len(batch_list)-1):\n",
    "            start_index = batch_list[i]\n",
    "            end_index = batch_list[i+1]\n",
    "            x_batch = x_train[start_index:end_index]\n",
    "            y_batch = y_train[start_index:end_index]\n",
    "            _, summary =sess.run([optimizer, all_summary], feed_dict={x: x_batch, y: y_batch})\n",
    "\n",
    "        cost = sess.run(cross_entropy, feed_dict={x: x_batch, y:y_batch})\n",
    "        \n",
    "        accuracy_train = sess.run(accuracy, feed_dict={x: x_train, y:y_train})*100\n",
    "        accuracy_test = sess.run(accuracy, feed_dict={x: x_test, y:y_test})*100\n",
    "        print('Epoch: {}, Accuracy_train: %{:.2f}, Accuracy_test: %{:.2f}, Cost: {:.3f}'.format(epoch+1,\n",
    "                                                                                                accuracy_train,\n",
    "                                                                                                accuracy_test,\n",
    "                                                                                                cost),end='')\n",
    "        print('\\r', end='')\n",
    "        tb_writer.add_summary(summary, epoch)\n",
    "        weight_repo.append(sess.run(w, feed_dict={x: x_batch, y:y_batch}))\n",
    "    \n",
    "    prediction = sess.run(y_p, feed_dict = {x: x_test, y: y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the final weights\n",
    "\n",
    "Note that weights that connect all the pixels into one output neuron corresponds to the class that the neuron represents. As can be seen below, the first neuron that classifies zero, assigns more weights to the pixels that form the number 0 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:53:43.306050Z",
     "start_time": "2019-03-16T20:53:42.865227Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "mnist_weight_check(weight_repo[-1], shape=[28,28], cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:54:09.653711Z",
     "start_time": "2019-03-16T20:54:09.627784Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "mnist_result_anim(x_test, y_test, prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using keras\n",
    " \n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. For more information, visit Keras.io. Currently, keras is a part of TensorFlow distribution so no need to install it separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:54:42.916491Z",
     "start_time": "2019-03-16T20:54:34.221355Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize data\n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
    "\n",
    "# Note: no need to transform the labels to one-hot format. Keras will do that for you\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Input layer for the firsrt layer the input shape is required\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "# Two hiddedn layer. Keras figures out the input shape based on the previous layer\n",
    "model.add(tf.keras.layers.Dense(units = 128, activation=tf.nn.relu)) # Rectified linear\n",
    "model.add(tf.keras.layers.Dense(units = 128, activation=tf.nn.relu)) \n",
    "\n",
    "# Output layer\n",
    "model.add(tf.keras.layers.Dense(units = 10, activation=tf.nn.softmax)) # output layer\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=100, validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:54:45.478696Z",
     "start_time": "2019-03-16T20:54:45.293131Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:54:50.706664Z",
     "start_time": "2019-03-16T20:54:50.498220Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "mnist_result_anim(x_test,y_test, prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer (Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:55:06.676392Z",
     "start_time": "2019-03-16T20:55:06.583641Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the NN for doing Linear Regression\n",
    "model = Sequential()\n",
    "\n",
    "# Using Dense as the simplest layer. We use one Neuron becuase we just need the slope (one weight) plus intercept (one bias)\n",
    "# The activation is lenear becuase we are doing linear regression\n",
    "model.add(Dense(1, input_shape=(1,), activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd') # Loss and optimizer (stochastic gradient descent)\n",
    "\n",
    "#   Fit/train the model\n",
    "model.fit(data_housing_train['HouseSize_std'], data_housing_train['HousePrice_std'], epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T14:16:46.706967Z",
     "start_time": "2019-02-25T14:16:46.701977Z"
    }
   },
   "outputs": [],
   "source": [
    "test_price_predicted = model.predict(data_housing_test['HouseSize_std'])\n",
    "test_price_predicted = test_price_predicted*price_std + price_mean\n",
    "test_price_predicted = test_price_predicted.reshape(len(data_housing_test['HouseSize_std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T14:16:47.655157Z",
     "start_time": "2019-02-25T14:16:47.307900Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(6,6))\n",
    "\n",
    "\n",
    "# Predicted Model\n",
    "ax.plot(data_housing_test.HouseSize, test_price_predicted, c='gray', lw=2, label = 'Predicted')\n",
    "# Actual test data\n",
    "ax.scatter(data_housing_test.HouseSize, data_housing_test.HousePrice, s = 20, label = 'Test Data')\n",
    "\n",
    "mean_squared_error = 0\n",
    "for price_point, size_point, price_pred in zip(data_housing_test.HousePrice, data_housing_test.HouseSize, test_price_predicted):\n",
    "    ax.plot([size_point, size_point], [price_point, price_pred], c='r', lw=1.5, ls ='--')\n",
    "    mean_squared_error += (price_point - price_pred)**2\n",
    "\n",
    "mean_squared_error = mean_squared_error/len(data_housing_test)\n",
    "rmse = np.sqrt(mean_squared_error)\n",
    "\n",
    "ax.text(0.1,.7, 'RMSE = {:.2f}'.format(rmse), transform = ax.transAxes, fontsize=16)\n",
    "\n",
    "ax.set_xlabel('House Size'); ax.set_ylabel('House Price')\n",
    "ax.legend(fontsize = 16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
